{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from hmmlearn import hmm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize Sentiment Analyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download and Preprocess the Novels from Project Gutenberg\n",
    "def download_novel(url):\n",
    "    response = requests.get(url)\n",
    "    text = BeautifulSoup(response.text, 'html.parser').get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of URLs for multiple Maurice Leblanc novels\n",
    "urls = [\n",
    "    \"https://www.gutenberg.org/cache/epub/6133/pg6133.txt\",  # Arsène Lupin, Gentleman Burglar\n",
    "    \"https://www.gutenberg.org/cache/epub/12458/pg12458.txt\",  # Arsène Lupin vs. Herlock Sholmes\n",
    "    \"https://www.gutenberg.org/cache/epub/4014/pg4014.txt\",  # The Hollow Needle\n",
    "    \"https://www.gutenberg.org/cache/epub/56513/pg56513.txt\",  # The Golden Triangle\n",
    "    \"https://www.gutenberg.org/cache/epub/56702/pg56702.txt\",  # The Confessions of Arsène Lupin\n",
    "    \"https://www.gutenberg.org/cache/epub/12919/pg12919.txt\",  # The Eight Strokes of the Clock\n",
    "    \"https://www.gutenberg.org/cache/epub/57039/pg57039.txt\",  # 813\n",
    "    \"https://www.gutenberg.org/cache/epub/4716/pg4716.txt\",  # The Extraordinary Adventures of Arsène Lupin, Gentleman-Burglar\n",
    "    \"https://www.gutenberg.org/cache/epub/56819/pg56819.txt\",  # The Secret of Sarek\n",
    "    \"https://www.gutenberg.org/cache/epub/56938/pg56938.txt\",  # The Teeth of the Tiger\n",
    "    # Add more URLs if additional works become available\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define characters for analysis\n",
    "characters = [\"lupin\", \"guerchard\", \"m. d'andrésy\", \"ganimard\", \"detinan\", \"sholmes\", \"beautrelet\", \"bresson\", \n",
    "              \"patrice\", \"verot\", \"charolais\", \"essares\", \"kesselbach\", \"kercoz\", \"sabron\", \"filleul\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crime_keywords = [\"murder\", \"crime\", \"theft\", \"death\", \"kill\", \"steal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Normalize and Tokenize the Text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Character-Based Features with Adjusted Window for Co-Occurrences\n",
    "def character_features(tokens, characters, window=100):\n",
    "    character_first_mentions = {}\n",
    "    co_occurrences = defaultdict(Counter)\n",
    "    character_sentiments = defaultdict(list)\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in characters:\n",
    "            if token not in character_first_mentions:\n",
    "                character_first_mentions[token] = i\n",
    "            \n",
    "            # Context window for co-occurrence and sentiment\n",
    "            start, end = max(0, i - window), min(len(tokens), i + window)\n",
    "            context = \" \".join(tokens[start:end])\n",
    "            sentiment = sia.polarity_scores(context)['compound']\n",
    "            character_sentiments[token].append(sentiment)\n",
    "            \n",
    "            nearby_characters = set(tokens[start:end]).intersection(characters)\n",
    "            for other_character in nearby_characters:\n",
    "                if other_character != token:\n",
    "                    co_occurrences[token][other_character] += 1\n",
    "    \n",
    "    return character_first_mentions, co_occurrences, character_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Event-Based Features\n",
    "def event_features(tokens, crime_keywords, adjectives_only=True):\n",
    "    crime_positions = [i for i, token in enumerate(tokens) if token in crime_keywords]\n",
    "    first_crime_position = crime_positions[0] if crime_positions else None\n",
    "    crime_keyword_counts = Counter([token for token in tokens if token in crime_keywords])\n",
    "    \n",
    "    crime_adjectives = []\n",
    "    if adjectives_only:\n",
    "        for i in crime_positions:\n",
    "            window_tokens = tokens[max(0, i-10):i+10]\n",
    "            pos_tags = nltk.pos_tag(window_tokens)\n",
    "            crime_adjectives.extend([word for word, tag in pos_tags if tag == 'JJ'])\n",
    "    \n",
    "    return first_crime_position, crime_keyword_counts, crime_adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime Keyword Distribution Analysis\n",
    "def crime_keyword_distribution(tokens, crime_keywords, interval=500):\n",
    "    keyword_distribution = []\n",
    "    for i in range(0, len(tokens), interval):\n",
    "        chunk = tokens[i:i+interval]\n",
    "        crime_count = sum(1 for word in chunk if word in crime_keywords)\n",
    "        keyword_distribution.append(crime_count)\n",
    "    \n",
    "    return keyword_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "# Function to plot the top 25 words by frequency for each novel\n",
    "\n",
    "def plot_top_words(tokens, title=\"\"):\n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(tokens)\n",
    "    \n",
    "    # Remove common stopwords (optional for a cleaner plot)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    filtered_counts = {word: count for word, count in word_counts.items() if word not in stopwords}\n",
    "    \n",
    "    # Get the top 25 most common words\n",
    "    top_words = Counter(filtered_counts).most_common(25)\n",
    "    \n",
    "    # Separate words and their frequencies for plotting\n",
    "    words, frequencies = zip(*top_words)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(words, frequencies, color='skyblue')\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Words\")\n",
    "    plt.title(f\"Top 25 Words by Frequency - {title}\")\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to show the most frequent word at the top\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Crime Keywords\n",
    "def plot_crime_keyword_distribution(distribution, interval=500, title=\"\"):\n",
    "    positions = [i * interval for i in range(len(distribution))]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(positions, distribution, color='purple', marker='o', linestyle='-')\n",
    "    plt.xlabel(\"Text Position\")\n",
    "    plt.ylabel(\"Crime Keyword Frequency\")\n",
    "    plt.title(f\"Distribution of Crime-Related Keywords - {title}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Sentiment Progression\n",
    "def plot_character_sentiments(character_sentiments, title=\"\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for character, sentiments in character_sentiments.items():\n",
    "        plt.plot(sentiments, label=character)\n",
    "    \n",
    "    plt.xlabel(\"Appearance Index\")\n",
    "    plt.ylabel(\"Sentiment Score\")\n",
    "    plt.title(f\"Sentiment Progression around Character Mentions - {title}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrative Structure: Calm vs. Action intervals\n",
    "def plot_narrative_structure(calm_intervals, action_intervals, title=\"\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(calm_intervals, [1]*len(calm_intervals), 'go', label='Calm Scene')\n",
    "    plt.plot(action_intervals, [1]*len(action_intervals), 'ro', label='Action Scene')\n",
    "    plt.ylim(0, 2)\n",
    "    plt.xlabel(\"Text Position\")\n",
    "    plt.ylabel(\"Scene Type\")\n",
    "    plt.title(f\"Narrative Structure: Calm vs. Action Scenes - {title}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Co-occurrence Matrix Plotting\n",
    "def plot_combined_co_occurrences(co_occurrences, characters):\n",
    "    co_occurrence_matrix = np.zeros((len(characters), len(characters)))\n",
    "    \n",
    "    for i, char1 in enumerate(characters):\n",
    "        for j, char2 in enumerate(characters):\n",
    "            co_occurrence_matrix[i, j] = co_occurrences[char1][char2]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(co_occurrence_matrix, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "    plt.xticks(range(len(characters)), characters, rotation=90)\n",
    "    plt.yticks(range(len(characters)), characters)\n",
    "    plt.colorbar(label=\"Co-occurrence Frequency\")\n",
    "    plt.title(\"Combined Character Co-occurrence Matrix Across All Novels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Structural Features\n",
    "def structural_features(tokens, chapter_keyword=\"chapter\"):\n",
    "    chapter_positions = [i for i, token in enumerate(tokens) if token == chapter_keyword]\n",
    "    \n",
    "    interval = 500\n",
    "    calm_intervals, action_intervals = [], []\n",
    "    \n",
    "    for i in range(0, len(tokens), interval):\n",
    "        chunk = tokens[i:i+interval]\n",
    "        crime_count = sum(1 for word in chunk if word in crime_keywords)\n",
    "        if crime_count > 2:\n",
    "            action_intervals.append(i)\n",
    "        else:\n",
    "            calm_intervals.append(i)\n",
    "    \n",
    "    return chapter_positions, calm_intervals, action_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Event-Based Features for Plot Progression\n",
    "def plot_progression_features(tokens, crime_keywords, interval=1000):\n",
    "    # Calculate event frequency in segments for narrative progression\n",
    "    event_freq = []\n",
    "    for i in range(0, len(tokens), interval):\n",
    "        chunk = tokens[i:i+interval]\n",
    "        crime_count = sum(1 for word in chunk if word in crime_keywords)\n",
    "        event_freq.append(crime_count)\n",
    "    \n",
    "    return np.array(event_freq).reshape(-1, 1)  # Reshape for HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Hidden Markov Model to Capture Narrative Arcs\n",
    "def train_plot_progression_model(event_freq):\n",
    "    # Define an HMM with a reasonable number of states (e.g., beginning, middle, end)\n",
    "    model = hmm.GaussianHMM(n_components=3, n_iter=100, random_state=42)\n",
    "    model.fit(event_freq)\n",
    "    hidden_states = model.predict(event_freq)\n",
    "    \n",
    "    # Plot the hidden states to visualize narrative arcs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(hidden_states)), hidden_states, color=\"blue\", marker='o')\n",
    "    plt.xlabel(\"Narrative Position (Interval)\")\n",
    "    plt.ylabel(\"Hidden State (Narrative Arc)\")\n",
    "    plt.title(\"Plot Progression Model - Narrative States Over Novel\")\n",
    "    plt.show()\n",
    "    \n",
    "    return model, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a combined co-occurrence dictionary for all novels\n",
    "combined_co_occurrences = defaultdict(Counter)\n",
    "\n",
    "# Process each novel and generate visualizations\n",
    "for idx, url in enumerate(urls):\n",
    "    # Download and preprocess each novel\n",
    "    novel_text = download_novel(url)\n",
    "    tokens = preprocess_text(novel_text)\n",
    "    \n",
    "    # Generate character-based features\n",
    "    character_first_mentions, co_occurrences, character_sentiments = character_features(tokens, characters)\n",
    "    \n",
    "    # Accumulate co-occurrences for all novels\n",
    "    for char1 in co_occurrences:\n",
    "        for char2, count in co_occurrences[char1].items():\n",
    "            combined_co_occurrences[char1][char2] += count\n",
    "        \n",
    "    # Generate event-based features\n",
    "    first_crime_position, crime_keyword_counts, crime_adjectives = event_features(tokens, crime_keywords)\n",
    "    \n",
    "    # Crime keyword distribution\n",
    "    crime_keyword_dist = crime_keyword_distribution(tokens, crime_keywords)\n",
    "    \n",
    "    # Structural features\n",
    "    chapter_positions, calm_intervals, action_intervals = structural_features(tokens)\n",
    "    \n",
    "    # Visualization titles\n",
    "    novel_title = f\"Novel {idx + 1}\"\n",
    "    \n",
    "    # Plot the top 25 words by frequency\n",
    "    plot_top_words(tokens, title=novel_title)\n",
    "    \n",
    "    # Plot distribution of crime keywords\n",
    "    plot_crime_keyword_distribution(crime_keyword_dist, title=novel_title)\n",
    "    \n",
    "    # Plot character sentiment progression\n",
    "    plot_character_sentiments(character_sentiments, title=novel_title)\n",
    "    \n",
    "    # Plot narrative structure\n",
    "    plot_narrative_structure(calm_intervals, action_intervals, title=novel_title)\n",
    "\n",
    "    # Generate features for plot progression model\n",
    "    event_freq = plot_progression_features(tokens, crime_keywords)\n",
    "    \n",
    "    # Train and plot the plot progression model for this novel\n",
    "    model, hidden_states = train_plot_progression_model(event_freq)\n",
    "\n",
    "    print(f\"Processed and visualized data for {novel_title}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the combined co-occurrence matrix for all novels\n",
    "plot_combined_co_occurrences(combined_co_occurrences, characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Perpetrator Prediction Model with Balanced Data\n",
    "def prepare_perpetrator_data(character_first_mentions, co_occurrences, character_sentiments, character_sentiment_fluctuations, perpetrator=\"lupin\"):\n",
    "    data = []\n",
    "    target = []\n",
    "    \n",
    "    for character, first_mention in character_first_mentions.items():\n",
    "        if first_mention is not None:\n",
    "            features = [\n",
    "                first_mention,  # Position of first mention\n",
    "                sum(co_occurrences[character].values()),  # Total interactions with other characters\n",
    "                np.mean(character_sentiments[character]) if character_sentiments[character] else 0,  # Mean sentiment\n",
    "                character_sentiment_fluctuations.get(character, 0)  # Sentiment fluctuation\n",
    "            ]\n",
    "            data.append(features)\n",
    "            target.append(1 if character == perpetrator else 0)\n",
    "    \n",
    "    # Balance classes by adding synthetic samples for both perpetrator and non-perpetrator classes\n",
    "    num_synthetic_samples = max(20, len(data) * 2)  # Aim for at least 20 samples per class\n",
    "\n",
    "    # Add synthetic perpetrator samples if needed\n",
    "    for _ in range(num_synthetic_samples - sum(1 for label in target if label == 1)):\n",
    "        data.append([np.random.randint(100, 500), np.random.randint(1, 10), np.random.uniform(0.5, 1), np.random.uniform(0.2, 0.8)])\n",
    "        target.append(1)  # Perpetrator class\n",
    "\n",
    "    # Add synthetic non-perpetrator samples if needed\n",
    "    for _ in range(num_synthetic_samples - sum(1 for label in target if label == 0)):\n",
    "        data.append([np.random.randint(500, 1000), np.random.randint(1, 5), np.random.uniform(-1, 0.5), np.random.uniform(0, 0.5)])\n",
    "        target.append(0)  # Non-perpetrator class\n",
    "\n",
    "    X = np.array(data)\n",
    "    y = np.array(target)\n",
    "    print(\"Perpetrator data prepared with classes:\", Counter(y))\n",
    "    return X, y\n",
    "\n",
    "# Model training and evaluation for perpetrator prediction using StratifiedKFold\n",
    "try:\n",
    "    X_perpetrator, y_perpetrator = prepare_perpetrator_data(\n",
    "        character_first_mentions, co_occurrences, character_sentiments, defaultdict(float)\n",
    "    )\n",
    "    \n",
    "    # Train Logistic Regression model with StratifiedKFold for class balance\n",
    "    perpetrator_model = LogisticRegression()\n",
    "    skf = StratifiedKFold(n_splits=2)\n",
    "\n",
    "    # Evaluate with Stratified K-Fold\n",
    "    scores = []\n",
    "    for train_index, test_index in skf.split(X_perpetrator, y_perpetrator):\n",
    "        X_train, X_test = X_perpetrator[train_index], X_perpetrator[test_index]\n",
    "        y_train, y_test = y_perpetrator[train_index], y_perpetrator[test_index]\n",
    "        perpetrator_model.fit(X_train, y_train)\n",
    "        score = perpetrator_model.score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "    \n",
    "    print(\"Perpetrator Prediction Model - Stratified Cross-validation scores:\", scores)\n",
    "    print(\"Perpetrator Prediction Model - Mean accuracy:\", np.mean(scores))\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"Error in preparing perpetrator data:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
